<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[你应该知道的10个关于ES2020的新特性]]></title>
    <url>%2FJavaScript%2FWeb%2FES2020%2F</url>
    <content type="text"><![CDATA[作者：Mehul Mohan原文：https://www.freecodecamp.org/news/javascript-new-features-es2020/ 好消息：ES2020的新特性都已经完成了！这意味着我们现在对新的 JavaScript 规范 ES2020 的变化有了完整的了解。So，让我们看看都有哪些变化把。 1：BigIntBigInt 是 JavaScript 最值得期待的特性之一，它终于来了。实际上，它允许开发人员在其 JS 代码中能使用大得多的整数表示形式进行数据处理。 目前，我们可以在 JavaScript 中能存储的整数最大值是 pow(2, 53) - 1。但是 BigInt 实际上允许你超过这个范围。 不过，就像你上面看到的一样，你需要在数字后面添加一个 n。额外的 n 说明这是一个 BigInt，并且告诉 JS 引擎应该区别对待（v8 引擎或者其他使用的任意引擎）。 由于过去使用的标准是 IEEE754（不支持这种大小的数字），因此 BigInt 的改进不向后兼容。 2：动态 ImportJavaScript 中的动态导入使我们可以选择将 JS 文件作为模块自然地动态导入应用程序中。这就好像当前使用的 Webpack 和 Babel 一样，有相同的效果。 这个特性将有助于我们写出按需加载的代码（通常称为代码拆分），而不会增加 webpack 或其他模块打包器的开销。如果需要的话，还可以有条件地在 if-else 块中加载代码。 由于实际上导入的是一个模块，因此永远不会污染全局命名空间。 1234if(myCondition) &#123; const module = await import('./dynamicmodule.js') module.addNumbers(3, 4, 5)&#125; 3：空值合并空值合并增加了真正检验空值而不是虚值的能力。那么你可能又会问，什么是空值，什么是虚值？ 在 JavaScript 中，有一些值会被认定为 false，就比如说空字符串，数字 0，null，undefined，false和 NaN 等等。 但是，很多时候你可能想判断一个变量是否为空值，空值是指变量为 undefined 或者 null，而这个变量也可能为空字符串或者甚至是 false。 上述的这种情况，你将可以用上新的空值合并运算符 ?? 了。 你可以清楚的看到上面的 4 种情况，|| 运算符始终返回的是真值，而 ?? 运算符始终返回的是一个非空值。 4：可选链可选链语法允许我们读取一个被连接对象的深层次的属性的值而无需明确校验链条上每一个引用的有效性。如果存在，那就太好了，正常返回对应属性值。如果不存在，将直接返回 undefined。 可选链不仅适用于对象属性，而且适用于函数调用和数组。使用起来非常方便！下面展示一些例子。 5：Promise.allSettledPromise.allSettled 方法接收一个 Promise 的数组（译注：准确的说接收的是一个可迭代对象），并且仅当所有的这些 Promise 都被决议（不管 Promise 的最终状态是成功达成或者被拒绝）之后才会最终完成。 6：String#matchAllmatchAll 是一个添加到 String 原型对象的关于正则表达式的新方法。这个方法的返回值是一个迭代器，可以一个接一个的返回所有的捕获组。让我们看一个简单的例子： 7：globalThis如果你写过一些能运行在 Node 环境，浏览器环境并且也能运行在 web-worker 中的跨平台 JS 代码，你就会觉得持有全局对象是一件很麻烦的事。 这是因为在浏览器中全局对象是 window，在 Node 环境中是 global，而在 web worker 里是 self。如果有更多的运行时环境，那么全局对象也将是不同的。 所以你不得不有自己的实现来检测运行时，在实现中使用正确的全局对象。这种方式一直延续到现在。 ES2020 给我们带来了 globalThis，不管我们的代码在什么环境下执行，globalThis 始终指向的都是全局对象。 8：模块命名空间导出在 JavaScript 模块中，已经可以使用以下语法了： 1import * as utils from './utils.mjs' 但是，到目前为止，还没有类似的导出语法： 1export * as utils from './utils.mjs' 等效于以下代码： 12import * as utils from './utils.mjs'export &#123; utils &#125; 9：明确定义 for-in 顺序ECMA 规范未指定 for (x in y) 应按哪个顺序运行。即使在这之前浏览器都已经自己实现了固定的顺序，但 ES2020 已正式将其标准化。 10：import.metaimport.meta 对象是由 ECMAScript 实现并创建的，带有 null 的原型对象。 假设一个模块 module.js： 1&lt;script type="module" src="module.js"&gt;&lt;/script&gt; 你可以使用 import.meta 对象访问有关模块的元信息： 1console.log(import.meta); // &#123; url: "file:///home/user/module.js" &#125; 它返回一个带有 url 属性的对象，指明模块的基本URL。可以是获取的脚本的URL（指外部脚本），也可以是包含文档的文档基URL（指内联脚本）。 同内容的视频]]></content>
      <categories>
        <category>JavaScript</category>
        <category>Web</category>
      </categories>
      <tags>
        <tag>ES2020</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Web 安全攻防总结]]></title>
    <url>%2FWeb%2Fweb-security%2F</url>
    <content type="text"><![CDATA[前言开发的大部分 Web 应用都是服务于内网，导致 Web 的安全问题并没有仔细的考虑和实践过。这次借着部门内部分享的机会好好总结下 Web 安全攻防的相关知识，避免以后踩雷。本文主要分析常见的几种的 Web 安全问题，并探讨其原理和防御策略。 SQL 注入SQL 注入是指 Web 应用程序对用户输入的合法性判定或者过滤不严，导致攻击者可以在管理员不知情的情况下实现非法操作，从而执行非授权的数据库查询，获取用户信息等。 SQL 攻击原理 盗取网站敏感信息假设存在某种应用场景，用户在前端输入ID，后台根据用户的输入按照如下的规则拼接 SQL ， 然后把查询结果返回到用户： 12345const querySQL = ` SELECT first_name, last_name FROM users WHERE user_id = '$&#123;userID&#125;'`; 普通用户的输入可以得到如下结果： 但是攻击者可不会乖乖的输入正常的用户ID，比如攻击者输入 x&#39; union select database(),user()# 进行查询，这时查询语句就变成了这样： 1SELECT first_name, last_name FROM users WHERE user_id = 'x' union select database(),user()#' database() 将会返回当前网站所使用的数据库名字 user() 将会返回执行当前查询的用户名 x 的值是随意构造的 这时用户查询到的是管理员不希望用户知道的信息，当然除了可以获取数据库的名字和查询用户名之外，还可以获取其他的一些信息，比如通过 version() 获取数据库版本，@@version_compile_os 获取当前操作系统等等。 绕过登录验证假设前端的登录表单如下： 12345&lt;form action="/login" method="POST"&gt; &lt;p&gt;用户: &lt;input type="text" name="username" /&gt;&lt;/p&gt; &lt;p&gt;密码: &lt;input type="password" name="password" /&gt;&lt;/p&gt; &lt;p&gt;&lt;input type="submit" value="登陆" /&gt;&lt;/p&gt;&lt;/form&gt; 后端 SQL 语句的拼接可能是如下这样： 1234567const querySQL = ` SELECT * FROM user WHERE username='$&#123;username&#125;' AND psw='$&#123;password&#125;'`;// 接下来就是执行 sql 语句... 正常的用户输入，则会执行类似如下这样的 SQL 语句，只有当用户名和密码都匹配的时候，才能正常登录。 1SELECT * FROM user WHERE username='admin' and password='WUs&amp;Ez'; 但是现实情况是，并不是所有用户都能乖乖的输入用户名和密码，比如攻击者进行如下输入： 这时，后台实际执行的 SQL 就变成了下面这样： 1SELECT * FROM user WHERE username='admin' OR 1 = 1 --' and password='111111'; 按照 MySQL 的语法，-- 后面的内容相当于注释会被忽略，这样攻击者在不用知道密码的情况下，可以使用 admin 的身份绕过登录验证，直接成功登录。 预防 SQL 注入SQL 注入的核心就是用户的非法输入拼接到了后端的 SQL 中，所以防范的核心在于永远不相信用户的输入，对于用户的输入信息进行充分转义。除此之外也还是不够的，还需要注意以下几点： 严格限制Web应用的数据库的操作权限，给此用户提供仅仅能够满足其工作的最低权限，从而最大限度的减少注入攻击对数据库的危害 充分转义，或者限制用户输入数据库特殊字符（’，”，\，&lt;，&gt;，&amp;，*，; 等） 使用参数化查询，现代语言基本都有提供参数化查询的接口，比如 Node.js 中的 mysqljs 库的 query 方法中的 ? 占位参数 1mysql.query(`SELECT * FROM user WHERE username = ? AND psw = ?`, [username, psw]); 在应用发布之前建议使用专业的 SQL 注入检测工具进行检测，以及时修补被发现的 SQL 注入漏洞。网上有很多这方面的开源工具，例如 sqlmap、SQLninja 等 避免网站打印出 SQL 错误信息，比如类型错误、字段不匹配等，把代码里的 SQL 语句暴露出来，以防止攻击者利用这些错误信息进行 SQL 注入 公司代码中的 SQL 语句属于高级别的机密，特别是关键的表和库，不应该随意暴露，容易被别有用心的人利用。 命令行注入命令行注入漏洞，指的是攻击者能够通过 HTTP 请求直接侵入主机，执行攻击者预设的 shell 命令，听起来好像匪夷所思，这往往是 Web 开发者最容易忽视但是却是最危险的一个漏洞之一。 命令行注入的原理和 SQL 注入的原理是一样的，不过从后台拼接 SQL 语句转换成了后台拼接 shell 命令，其预防方法也是一样的。不过命令行注入带来的危害甚至更大。所以站点服务调用 shell 脚本时应该要限制执行权限。 XSSXSS 全称为Cross Site Scripting，为了和 CSS 分开简写为 XSS ，中文名为跨站脚本。该漏洞发生在用户端，是指在渲染过程中发生了不在预期过程中的 JavaScript 代码执行。XSS通常被用于获取 Cookie、以受攻击者的身份进行操作等行为。 XSS 的本质是：恶意代码未经过滤，与网站正常的代码混在一起；浏览器无法分辨哪些脚本是可信的，导致恶意脚本被执行。 存储型 XSS存储型的 XSS 一般存在于 Form 表单提交等交互功能，如发帖留言等。也就是说攻击者的恶意代码已经存储到了网站的数据库中，当前端页面获取数据进行展示时，恶意代码就会被执行。 存储型的 XSS 不需要诱骗用户进行点击，只需要找到网站漏洞，进行注入即可，而且一旦攻击成功，危害面就会非常广。不过这种 XSS 攻击的成本也还是很高，需要站点同时满足以下的条件： POST 请求的表单，前后端都没有进行转义，直接进行入库 前端请求的数据直接来自数据库，没有进行转义 前端生成 DOM 时，没有对后端返回的数据进行检查 存储型的 XSS 有以下几个特点： 持久性，植入在数据库中 危害面广，甚至可以让用户机器变成 DDoS 攻击的肉鸡 盗取用户敏感私密信息 反射型 XSS反射型 XSS 漏洞常见于通过 URL 传递参数的功能，如网站搜索、跳转等，由于需要用户主动打开恶意的 URL 才能生效，攻击者往往会结合多种手段诱导用户点击（比如邮件发送攻击链接等）。XSS 攻击的步骤大致如下图所示： POST 的内容也可以触发反射型 XSS，只不过其触发条件比较苛刻（需要构造表单提交页面，并引导用户点击），所以非常少见。 假设服务端的代码如下图所示，服务端根据 URL 参数返回搜索的值和搜索结果，这样的场景跟百度或者谷歌的搜索框是一样的： 12345678const getData = ctx.query;ctx.body = ` &lt;div&gt; &lt;p&gt;你的搜索是：&lt;/p&gt; $&#123;getData.search&#125; &lt;p&gt;搜索结果是：... ...&lt;/p&gt; &lt;/div&gt; `; 乍一看好像没什么问题，正常的请求应该类似是这样的：http://api.ceshi.cn/xss?search=NormalData，这时前端的 HTML 将正常显示。 12345&lt;div&gt; &lt;p&gt;你的搜索是：&lt;/p&gt; NormalData &lt;p&gt;搜索结果是：... ...&lt;/p&gt;&lt;/div&gt; 然而攻击者可以构造一条这样的请求，诱导普通用户进行点击：http://api.ceshi.cn/xss?search=&lt;img onerror=&quot;alert(&#39;执行侵入代码&#39;)&quot; src=&quot;x&quot;&gt;，这时服务端会返回代码侵入代码的脚本到用户，并且执行。 12345&lt;div&gt; &lt;p&gt;你的搜索是：&lt;/p&gt; &lt;img onerror="alert('执行侵入代码')" src="x"&gt; &lt;p&gt;搜索结果是：... ...&lt;/p&gt;&lt;/div&gt; 可以看到 HTML 中被插入了侵入的节点，因为 img 标签在 src 属性无效的情况下，会执行 onerror 中的 js 代码，通常攻击者会在这段脚本中获取用户登录的 cookie 等信息。 DOM 型 XSS反射型的 XSS 攻击是属于服务端的安全漏洞，不过通过创建不受控的 DOM 节点的方式进行攻击也可以完全在浏览器端完成，这种完全通过浏览器端取出和执行恶意代码的攻击方式就属于 DOM 型的 XSS。 特别是目前大量流行的框架都是前后端分离， DOM 型的 XSS 会更加的常见。 DOM 型的 XSS 攻击原理同反射型的类似，只需要把【Web 应用程序】改成 【客户端程序】 即可。这里不进行过多的赘述了。 XSS 攻击的预防通过以上的分析，可以得到，XSS 攻击有两大要素： 攻击者提交恶意代码 浏览器执行恶意代码 所以防御可以从以上两点入手，包括检测用户输入和检测 HTML 的输出： 输入检测，用户的所有输入都不应该信任。不过这种防范策略是有局限性的，更多的是对于有明确的输入类型的数据进行检测，比如数字，URL，电话、邮箱等内容。但是大部分时候我们并没有明确的输入类型限定，也不希望转义用户的输入，而是保留用户的原始数据。这时候就需要检测 HTML 的输出环节了。 采用前后端分离的方式，数据和代码分割开。 在前端渲染时明确的告诉浏览器，从后端获取的数据以纯文本的方式进行展示，或者明确的设置属性、样式等，这样可以避免存储型和反射型的 XSS 攻击，但依然需要考虑 DOM 型的XSS攻击。 后端对输出的 HTML 进行充分转义。这样可以避免存储型和反射型的 XSS 攻击。 前端对输出的 HTML 进行充分转义。DOM 型 XSS 攻击，实际上就是因为前端的 JavaScript 代码不够严谨造成的。 其他 XSS 防范的措施： 第一点就是 CSP 策略。Content Security Policy，简称 CSP。顾名思义，这个规范与内容安全有关，主要是用来定义页面可以加载哪些资源，减少 XSS 的发生。CSP的策略可以通过 HTTP 头或者 meta 元素进行定义，具体的用法，网上有很多其他教程，这里不做叙述了。 通过严格的 CSP 策略在 XSS 防范中可以起到一下作用： 禁止加载外域的代码，可以有效防止复杂的攻击逻辑 禁止外域提交，也就是说就算站点被攻击，截获的用户信息也没法传输到攻击者 利用 Content-Security-Policy-Report-Only 字段可以上报并及时发现 XSS 漏洞 第二点是限制内容长度，通过 CSP 策略已经限制了外域的代码，再通过限制用户输入长度，达到限制侵入脚本大小的目的，可以大大增加攻击的难度。 第三点是对于可能存在的风险提交进行验证码校验。 第四点是对于关键的 Cookie 增加 HTTPOnly 的限制，这样就算攻击者成功注入恶意脚本，也无法窃取此 Cookie。 可以看到 github.com 就使用了上述的这些策略。 CSRFCSRF（Cross-site request forgery）跨站请求伪造：攻击者诱导受害者进入第三方网站，在第三方网站中，向被攻击网站发送跨站请求。利用受害者在被攻击网站已经获取的注册凭证，绕过后台的用户验证，达到冒充用户对被攻击的网站执行某项操作的目的。一个典型的 CSRF 攻击包括了如下的这样一些步骤。 GET 类型的 CSRF假设某站点 A 发帖的操作，是通过已登录用户点击对应帖子的发送按钮，向服务端发送如下 GET 请求来实现的： 1http://api.example.qx/post/delete?post_id=1003&amp;content=hello 而在另一个危险的站点 B，存在这样一段 HTML 代码： 1&lt;img src=http://api.example.qx/post/delete?post_id=1003&amp;content=csrf&gt; 那么当 A 站点的已登录用户 C 访问站点 B 时，浏览器会自动请求 img 标签的 src 地址，并且会携带 A 站点的 Cookie 信息进行请求，达到攻击者利用 C 的身份信息在 A 站点发帖的目的，甚至 C 都不知道自己被 B 所利用。 POST 类型的 CSRF继续说到上面发帖的例子，A 站点发现漏洞进行了改造，将发帖的接口由 GET 请求改成了 POST 请求。不过这样也是挡不住入侵者的攻击的，危险站点与时俱进，也改进了入侵代码： 12345&lt;form action="http://api.example.qx/post/delete" method=POST&gt; &lt;input type="hidden" name="post_id" value="1003" /&gt; &lt;input type="hidden" name="content" value="csrf" /&gt;&lt;/form&gt;&lt;script&gt; document.forms[0].submit(); &lt;/script&gt; 可以看出，只要用户访问危险站点B，表单就会自动提交。POST 类型的 CSRF 虽然比 GET 类型要严格些，但仍然不复杂，不能起到抵御 CSRF 的作用。 CSRF 攻击的特点 攻击一般发生在第三方网站，而不是被攻击的网站。所以被攻击的网站无法进行阻止。 攻击者只是利用受害者的登录凭证，而非窃取数据。 CSRF 形式可以各种各样，比如图片 URL、超链接、CORS、Form 提交等。 CSRF 攻击一般发生在第三方网站，所以通常都是需要跨域的。 CSRF 的防护策略根据 CSRF 攻击的特点，被攻击的网站是没法阻止攻击的发生的，只能提高自身防御力，实际上就是阻止攻击者进行跨域请求或者阻止攻击者利用用户的登录凭证就可以了。 阻止不明外域的访问 同源检测（Origin Header 和 Referer Header） Samesite Cookie 提交时要求附加本域才能获取的信息 CSRF Token 双重Cookie验证 重要请求增加验证码环节，这样的接口不应该太多，比较影响用户体验参考文档常见 Web 安全攻防总结前端安全系列（一）：如何防止XSS攻击前端安全系列（二）：如何防止CSRF攻击]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Web 安全</tag>
        <tag>XSS</tag>
        <tag>CSRF</tag>
        <tag>SQL 注入</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 从并发编程谈协程]]></title>
    <url>%2FPython%2Fcoroutine%2F</url>
    <content type="text"><![CDATA[从一个实际案例说起当我们希望从网上爬取一些有用的信息时，就可能会写出类似以下这样的代码，不过这里简化了任务的逻辑。 123456789101112131415161718192021222324252627282930313233343536import requestsimport timefrom functools import wrapsdef func_time(func): @wraps(func) def new_func(*args, **kwargs): print(f'start &#123;func.__name__&#125;') start = time.time() result = func(*args, **kwargs) stop = time.time() print(f'finish &#123;func.__name__&#125;, and taking &#123;format(stop - start, "0.2f")&#125; s') return result return new_funcdef do_work(url): requests.get(url)@func_timedef do_works_serial(tasks): for task in tasks: do_work(task)def main(): tasks = [ "https://movie.douban.com/subject/5912992/", "https://movie.douban.com/subject/30170448/", "https://movie.douban.com/subject/30334073/", "https://movie.douban.com/subject/1292064/", "https://movie.douban.com/subject/21937445/", ] do_works_serial(tasks)# output:# start do_works_serial# finish do_works_serial, and taking 3.59 s 上面的代码处理方式是依次进行网络请求，上一个请求返回之后再进行下一个请求。 这种方式在需要处理的小任务不多的时候，倒也是可行的。但是处理方式非常笨拙，会有大量的时间耗费在等待请求的返回，没有充分地利用到计算机的资源，所以通常对于这样的需求，最容易想到的就是多线程和多进程，就像下面的代码一样。 123456789101112131415161718192021222324252627@func_timedef do_works_muti_threads(tasks): threads = set() for task in tasks: t = threading.Thread(target=do_work, args=(task,)) t.start() threads.add(t) # 等待所有线程的结束 for t in threads: t.join()@func_timedef do_works_muti_processes(tasks): processes = set() for task in tasks: t = Process(target=do_work, args=(task,)) t.start() processes.add(t) # 等待所有进程的结束 for t in processes: t.join()# output# start do_works_muti_threads# finish do_works_muti_threads, and taking 1.38 s# start do_works_muti_processes# finish do_works_muti_processes, and taking 1.61 s 可以发现，不论是多线程或者多进程的方案，都会提升程序的效率（当然这里的计时并不严谨，因为网络波动会影响耗时），实际上多线程或者多进程的也是软件工程解决并发问题常用的手段。 不过多线程或者多进程的方案并不是万能的，这两种方案存在一些问题，第一，会占用额外的系统资源，而且进程会占用更多的资源，互联网领域有名的 C10K 问题，就印证了这点，频繁的上下文切换带来了大量的资源消耗。第二，多线程会有同步共享资源的问题，使用锁无疑又会增加资源的消耗，而多进程的方案会有进程间通信问题。 随着技术的进步，人们又推出了协程的概念，可以轻松应对上述问题。而 Python 也在语言层面提供了支持，使得协程即提高了程序运行效率，又保证了代码的可读性，本文的示例代码都是基于 Python 3.7的（另一种基于生成器的协程，使用 @asyncio.coroutine 装饰，不过会在 Python 3.10 中移除，这里不做讨论了）。 使用协程解决并发问题说了那么多，Python 3.7 中使用协程是如何解决上面提到案例呢？ 1234567891011121314151617181920212223242526272829303132333435import asyncioimport aiohttpimport threadingasync def do_work_async(url): print(f'&#123;url&#125;:&#123;threading.currentThread().ident&#125;') async with aiohttp.ClientSession() as session: async with session.get(url) as _: print(f'&#123;url&#125; done')async def coro_func(tasks): tasks = (asyncio.create_task(do_work_async(task)) for task in tasks) print('before tasks') await asyncio.gather(*tasks) print('finish tasks')@func_timedef do_works_coroutine(tasks): asyncio.run(coro_func(tasks))# output# start do_works_coroutine# before tasks# https://movie.douban.com/subject/5912992/:3856# https://movie.douban.com/subject/30170448/:3856# https://movie.douban.com/subject/30334073/:3856# https://movie.douban.com/subject/1292064/:3856# https://movie.douban.com/subject/21937445/:3856# https://movie.douban.com/subject/1292064/ done# https://movie.douban.com/subject/21937445/ done# https://movie.douban.com/subject/5912992/ done# https://movie.douban.com/subject/30334073/ done# https://movie.douban.com/subject/30170448/ done# finish tasks# finish do_works_coroutine, and taking 1.02 s 可以发现代码只有短短的10来行，不过对于没有接触过协程的同学来说，不太好理解，所以针对上面代码，这里稍作梳理，更多关于 async 的介绍和用法应该参考官方文档。 概念梳理 协程函数: 定义形式为 async def 的函数 协程对象: 调用 协程函数 所返回的对象 await: 等待协程的返回 协程函数直接调用并不会被执行，而是返回协程对象，另外还会得到一个警告 coroutine &#39;xxx&#39; was never awaited ，所以说协程函数与 await 通常是配套使用的。 asyncio.run： 该函数接收的参数是协程，负责管理整个事件循环，调用时会创建一个新的事件循环并在结束时关闭之，它应当被用作 asyncio 程序的主入口点，理想情况下应当只被调用一次。 asyncio.create_task： 将协程打包并返回一个 Task 对象，而Task 对象被用来在事件循环中运行协程，可以通过 Task 对象取消正在运行的任务，或者是获取任务是否已经完成等信息。 asyncio.gather： 并发的运行 Task 对象 运行逻辑探讨了解完上面这些内容之后，我们再回过头来分析下代码。 asyncio.run(coro_func(tasks)) 开始进入事件循环。 task 依次被创建，进入事件循环并开始等待运行；打印 “before tasks”。 await asyncio.gather，开始‘并发’的执行 task，通过打印的 log 发现虽说是‘并发’，但实际上依然是单线程运行。程序运行到 aiohttp.ClientSession() 或者 session.get(url) 时并不会阻塞，而是切出当前任务，由事件调度器开始调度下一个任务。 当所有的任务都进入了等待状态之后，事件调度器也会暂停，直到有任务返回。 有任务请求返回则会获取到调度器的控制权，打印对应的 “url done”。 所有协程执行完成后，asyncio.run 返回，事件循环结束。 总结协程到底是什么以上说了这么多之后，应该了解了什么是协程，协程的工作机理，但是如果要一句话描述出来，还真不太好说。所以在 Stack Overflow 看到一句话描述，感觉挺合适的，就迁移过来了。 协程是一种通用控制结构，其中流的控制权在不同任务之间协同传递而不返回。 并发编程中，协程的优劣线程会比协程更重一些，需要操作系统知道线程的信息，操作系统会在合适的时机进行线程切换，这样做的优势就是代码简单，程序员友好，无需关系任务切换逻辑。 相比进程和线程，协程应该是最轻量的一种并发方案，任务切换完全由程序员自由控制，只有当上一个任务交出控制权下一个任务才能开始执行。对比线程，协程会更高效，但同时也存在一个问题，需要库提供者的支持，比如上面的例程中用到的 aiohttp ，如果开源社区没有的话，那么就需要自行实现了。 而多进程则是一种真正的并行方案，在多核 CPU 的机器上能同时运行多个进程。 总的来说，协程和线程更适合处理 I/O 密集的场景，特别是 Python 中的多线程实际上也只是单线程中执行；而对于 CPU 密集的场景来说，多进程、多机器、多处理器才能提高程序的运行速度。 代码地址 https://github.com/Jacksonlike/blog_code/tree/master/coroutine 参考文档Stack Overflowasyncio 官方文档Coroutine in Pythonpython-parallel-programmning-cookbook]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>coroutine</tag>
        <tag>协程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 元类(Metaclass)]]></title>
    <url>%2FPython%2Fmetaclass%2F</url>
    <content type="text"><![CDATA[元类在 Pyhton 程序中，元类犹如一把利刃，使用得当能让框架开发者的代码更加优雅，调用更加便捷，而对于广大的应用层开发人员，元类设计不当，就会给代码引入不可预料的隐患，所以最好选择其他替代方案。 虽说应尽量避免使用元类，但也正是因为如此倒是勾起了我们探索的好奇心，下面一起来研究下元类的使用和 Python 实现元类的机理。 先通过下面两个实例，实际感受下 metaclass 到底能做什么。 单例模式12345678910111213141516171819202122class Singleton(type): def __init__(self, *args, **kwargs): self.__instance = None super().__init__(*args, **kwargs) def __call__(self, *args, **kwargs): if self.__instance is None: self.__instance = super().__call__(*args, **kwargs) return self.__instance else: return self.__instance# Example# Python3class Spam(metaclass=Singleton): def __init__(self): print('Creating Spam')# Python2# class Spam(Object):# __metaclass__ = Singleton 上述代码是通过元类实现的单例模式，对于调用者是使用方便的，而且完美的支持继承。示例： 123456789&gt;&gt;&gt; a = Spam()Creating Spam&gt;&gt;&gt; b = Spam()&gt;&gt;&gt; a is bTrue&gt;&gt;&gt; c = Spam()&gt;&gt;&gt; a is cTrue&gt;&gt;&gt; 也许你可能提出质疑，实现单例模式的方法很多，使用元类的优势在哪，这个问题在可以参见这里。对比其他的方式，元类的解决方案会更加优雅，代码更加清晰。 YAML load 函数的实现先看到下面的实例代码： 12345678910111213141516171819202122232425262728293031323334353637383940class Monster(yaml.YAMLObject): yaml_tag = u'!Monster' def __init__(self, name, hp, ac, attacks): self.name = name self.hp = hp self.ac = ac self.attacks = attacks def __repr__(self): return "%s(name=%r, hp=%r, ac=%r, attacks=%r)" % ( self.__class__.__name__, self.name, self.hp, self.ac, self.attacks)monster1 = yaml.load("""--- !Monstername: Cave spiderhp: [2,6] # 2d6ac: 16attacks: [BITE, HURT]""", Loader=yaml.Loader)print(type(monster1))print(monster1)print(yaml.dump(Monster(name='Cave spider', hp=[2, 6], ac=16, attacks=['BITE', 'HURT'])))# output&lt;class '__main__.Monster'&gt; Monster(name='Cave spider', hp=[2, 6], ac=16, attacks=['BITE', 'HURT'])!Monsterac: 16attacks:- BITE- HURThp:- 2- 6name: Cave spider 可以看到，Monster 类只是简单的继承了类 yaml.YAMLObject 之后，yaml.load 就可以自动将 yaml 的配置序列化成 Python 类了，这个设计使用简单，调用者完全没有任何冗余的转换代码 ，而 yaml.dump 也是在没有提前获取任何关于 Monster 类信息的情况下对 Monster 进行逆序列化。 那么如此强大的功能是怎么实现的呢？可以在 yaml 的 源码 中找到答案。 12345678910111213141516171819# 以下省略了无关代码class YAMLObjectMetaclass(type): def __init__(cls, name, bases, kwds): super(YAMLObjectMetaclass, cls).__init__(name, bases, kwds) if 'yaml_tag' in kwds and kwds['yaml_tag'] is not None: cls.yaml_loader.add_constructor(cls.yaml_tag, cls.from_yaml)class YAMLObject(metaclass=YAMLObjectMetaclass): yaml_loader = Loader @classmethod def from_yaml(cls, loader, node): return loader.construct_yaml_object(node, cls) class BaseConstructor: @classmethod def add_constructor(cls, tag, constructor): if not 'yaml_constructors' in cls.__dict__: cls.yaml_constructors = cls.yaml_constructors.copy() cls.yaml_constructors[tag] = constructor 从以上源码可以看到： 面向使用者的是 YAMObject ，其元类被设置为自定义的 YAMLObjectMetaclass 完成用户类 Monster 的定义后，就会初始化元类 YAMLObjectMetaclass Loader 继承自 BaseConstructor ，而类方法 add_constructor 用于绑定 tag 和 类信息 YAMLObjectMetaclass 判断到 Monster 是否拥有类属性 yaml_tag ，如果有就将 yaml_tag 和 Monster 这个类进行绑定，之后 load 时再根据 tag 索引到类信息，进行序列化。 元类的实现机理接下来从以下四点来说明进行说明。 一切皆对象Python 中所有类都继承自 object ，且所有的类都是 type 类的实例对象，当然也包括 type 本身。 12&gt;&gt;&gt; issubclass(type, object)True type 、class 和 object 具体的关系图如下： 所有的用户类都是 type 类的对象其实从上图已经能画出来了，所有的类都是对象，而且是 type 这个类的对象。 1234567&gt;&gt;&gt; class Test:... pass...&gt;&gt;&gt; type(Test)&lt;class 'type'&gt;&gt;&gt;&gt; type(Test())&lt;class '__main__.Test'&gt; 这里补充说明下，type() 有两个功能，当接收的参数只有一个时，返回类实例（传入的参数）所属的类，当接收的参数是三个时，返回的是一个新的 type 对象。HELP 创建类的过程实际上只不过是 type 类的 __call__ 运算符重载当我们定义一个类的时候，真正发生的事情是 Python 调用 type 的 __call__ 运算符。而 __call__ 运算符则进一步调用 __new__ 和 __init__ 。下面 show you code ，代码中两种创建类的方式是完全等效的。 12345678910111213# 常规方法创建类&gt;&gt;&gt; class Test1:... data = 1...&gt;&gt;&gt; instance1 = Test1()&gt;&gt;&gt; Test1, instance1, instance1.data(&lt;class '__main__.Test1'&gt;, &lt;__main__.Test1 object at 0x7fd528071630&gt;, 1)# 通过 type 创建类&gt;&gt;&gt; Test2 = type('Test2', (), &#123;'data': 1&#125;)&gt;&gt;&gt; instance2 = Test2()&gt;&gt;&gt; Test2, instance2, instance2.data(&lt;class '__main__.Test2'&gt;, &lt;__main__.Test2 object at 0x7fd528083748&gt;, 1) 所有自定义元类都继承 type当我们为某个类指定 metacalss 后，创建类的过程会变成如下代码所示： 123class = type(name, bases, dict)# 变成class = MyMetaclass(name, bases, dict) 上述 YAMl 的示例中，我们的 YAMLObject 类指定元类为 YAMLObjectMetaclass ，所以当代码运行到 Monster 的定义时，就会在 YAMLObjectMetaclass 的 __init__ 函数中偷偷进行注册。 代码地址 https://github.com/Jacksonlike/blog_code/tree/master/metaclass 参考文档Python CookbookPython进阶:metaclass谈Python一切皆对象]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>mateclass</tag>
        <tag>元类</tag>
        <tag>yaml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈 IPC 的几种方式（一）]]></title>
    <url>%2FUnix%2FPIPE%2F</url>
    <content type="text"><![CDATA[前言学而时习之不亦说乎，进程间通信相关的知识其实在很久以前就学习过，这方面的知识大部分工作根本用不着，但是对自己理解高阶代码有很大好处，故而借这个周末又翻了翻 APUE 相关章节，以下复习笔记，和一些自己的总结和思考。 进程间通信进程之间交换信息常见的途径有如下几种: 第一种通过 fork 生成子进程，父进程的所有信息，在子进程中都可以获取到，达到共享信息的目的，但是这种信息交换的方式对于没有亲属关系的进程之间是无效的。 第二种是通过约定，多个进程读写同一个文件的方式进行信息交换，这也是最容易被想到的方式。当程序及其简单的时候，这种方式实现起来非常简单倒是还有用武之地。但是代码量大起来，或者逻辑复杂之后，就显得比较低效，而且当不同进程同时读写文件的时候，容易发生混乱。 第三种就是今天要介绍的进程间通信 (InterProcess Communication, IPC)。常见的进程间通信方式包括如下几种，接下来逐一介绍。 无名管道 PIPE 有名管道 FIFO 信号 signal 消息队列 MSG 共享内存 SHM 信号量 SEM 套接字 Socket 无名管道 PIPE管道，命名就很形象，可以想象成一根水管连接两个进程，一边进水一边出水。 特征 没有名字，无法使用 open() 进行打开 因为第一个特点，没有名字，不能 open，所以只能用于有共同祖先进程的进程之间通信 半双工的工作方式，这一点也类似于真实的管道，读端写端分开 当要写入的数据量大于 PIPE_BUF （PIPE_BUF 的大小为4096字节）时，将不能保证写入的原子性，所以一般只使用于一对一的简单通信 使用管道是通过 pipe 函数创建的 12#include &lt;unistd.h&gt;int pipe(int fd[2]); // 创建成功返回 0，否则为 -1 函数返回成功后，fd[0] 成为读端，fd[1] 成为写端。fd[1] 读取 fd[0] 的输入。实际上 PIPE 并没有限制一定在两个不同的进程间使用，但是单个进程间的管道没有任何作用，也就不做讨论。下图截自 APUE 描述了管道的一般工作模型。 首先进程调用 pipe 函数，接着调用 fork 函数创建子进程，所以产生的子进程和父进程一样都能获取 PIPE 读端和写端。紧接着各自关闭所不需要的文件 ID，上图表示的是父进程传输信息到子进程，所以父进程关闭了读端，子进程关闭了写端。因为管道的半双工的特点，所以一般不会用一个管道进行双向的信息互换，更常见的是创建两个管道进行信息互换（如协同进程）。 简单的 demo 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;int main(int argc, char const *argv[])&#123; int fd[2]; if (-1 == pipe(fd)) &#123; perror("pipe error"); exit(1); &#125; pid_t pid = fork(); if (pid &gt; 0) &#123; /* 父进程 */ close(fd[0]); char *s = "hello, i am your father!"; // sleep(10); write(fd[1], s, strlen(s)); close(fd[1]); &#125; else if (pid == 0) &#123; /* 子进程 */ close(fd[1]); char buf[30]; bzero(buf, 30); read(fd[0], buf, 30); printf("read from father:%s\n", buf); close(fd[0]); &#125; else &#123; perror("fork error"); exit(1); &#125; return 0;&#125; /************************************************** * ouput：read from father:hello, i am your father! ****************************************************/ 注，PIPE 是一种特殊的文件，也可以使用一般的文件 IO 函数进行读写，但是同 FIFO、socket 一样是不能使用 lseek 之类的函数进行定位，因为它们不同于普通文件存在于硬盘、Flash 等块设备上，它们只存在于内存，由内核维护。 如果想查看进程的管道，可以将上述示例代码，sleep 取消注释再运行，可以看到以下结果。 shell 中的管道实际上有了以上的基础之后，shell中的管道倒是很容易理解。 下图说明了如何通过管道连接 find、grep 和 wc 命令，将 find 命令的标准输出重定向（通过 dup2 接口 ）到管道的写端，而将 grep 命令的标准输入指向管道的读端。grep 和 wc 之间也是同理。 有名管道 FIFO PIPE 最大的劣势就是只能使用在有共同祖先的进程间，应用场景单一，性能较弱，限制条件又太多。所以我们还需要一种更强大的管道 FIFO 。 特征 有名字，存在于普通的文件系统中，所以任何有权限的进程都可以通过文件 IO 函数读写 FIFO 具有写入原子性，多写者同时进行操作不会出现数据混乱 First In First Out 原则，最先写入就会最先被读出 使用1234567891011121314151617181920212223242526272829303132333435363738# read.c#define FIFO "/tmp/fifotest"int main(int argc, char const *argv[])&#123; if (access(FIFO, F_OK)) &#123; mkfifo(FIFO, 0644); &#125; int fifo = open(FIFO, O_RDONLY); // 只读的方式打开FIFO char msg[20]; memset(msg, 0, 20); read(fifo, msg, 20); printf("read from FIFO: %s\n", msg); return 0;&#125;# write.c#define FIFO "/tmp/fifotest"int main(int argc, char const *argv[])&#123; if (access(FIFO, F_OK)) &#123; mkfifo(FIFO, 0644); &#125; int fifo = open(FIFO, O_WRONLY); // 只写的方式打开FIFO char msg[20]; memset(msg, 0, 20); fgets(msg, 20, stdin); int n = write(fifo, msg, strlen(msg)); printf("sebded %d bytes to FIFO\n", n); return 0;&#125; 这是完整源码地址。代码执行效果如下。 示例代码非常的简单，不过有以下几点进行说明： read 和 write 是两个同时独立运行的进程。 刚开始运行 read 进程而还没运行 write 进程时，或者是运行了 write 进程且 read 进程还没有运行时，open 函数会被阻塞，因为管道文件（包括 FIFO、PIPE 和 socket ）不可以只有读端或者只有写端被打开。 除了打开管道的时候可能发生阻塞，在进行读写操作的时候也可能会发生阻塞，具体规则如下表所示。 当调用 open （或者调用 fcntl ）时，指定 O_NONBLOCK 的标志，则管道不会阻塞。具体规则与上述有所区别，如果需要可以查看 UNIX网络编程卷2 第四章 ，书中有详细描述。 参考文档对话 UNIX 探索管道Unix 环境高级编程UNIX网络编程 卷2]]></content>
      <categories>
        <category>Unix</category>
      </categories>
      <tags>
        <tag>IPC</tag>
        <tag>Unix</tag>
        <tag>进程通信</tag>
        <tag>C 语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2FLife%2Fhelloworld%2F</url>
    <content type="text"><![CDATA[Hello，这里是我的首篇博客~]]></content>
      <categories>
        <category>Life</category>
      </categories>
  </entry>
</search>
